{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#Vers. 1.0.0\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "#Should be above 3.5\n",
    "#print (sys.version)     \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import color\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "from Helpers import get_files, load_image, separate_imgs, get_next_batch_from_disk_RGB, get_next_batch_from_disk_RGB_Nocrop, get_next_batch_from_disk_RGB_Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_list = get_files(\"../Data/Extracted/*\", '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35950"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_list_test = get_files(\"../Data/Extracted/Test/\", '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building the Training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=seed, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(scope, x, kernel_size, stride=1):\n",
    "    W = weight_variable(kernel_size, \"conv_W_\" + scope)\n",
    "    b = bias_variable(kernel_size[3:], \"conv_b_\" + scope)\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return  tf.nn.relu(conv +  b)\n",
    "\n",
    "#From https://github.com/shekkizh/Colorization.tensorflow/blob/master/TensorflowUtils.py\n",
    "def conv2d_transpose_strided(scope, x, kernel_size, stride=2, output_shape=None):\n",
    "    W = weight_variable(kernel_size, \"deconv_W_\" + scope)\n",
    "    b = bias_variable(kernel_size[3:], \"deconv_b_\" + scope)\n",
    "    \n",
    "    if output_shape is None:\n",
    "        output_shape = x.get_shape().as_list()\n",
    "        output_shape[1] *= 2\n",
    "        output_shape[2] *= 2\n",
    "        output_shape[3] = W.get_shape().as_list()[2]\n",
    "        \n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "def batch_norm(scope, x, train=True, reuse=False):\n",
    "    return tf.contrib.layers.batch_norm(x, center=True, scale=True, updates_collections=None, is_training=train, trainable=True, scope=scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network_colorization_nilboyarch(x):\n",
    "    \n",
    "    #conv1\n",
    "    conv_num = 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), x, [3, 3, 1, 64], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 64], stride=2)\n",
    "    conv_num += 1\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "\n",
    "    #self.nilboy = temp_conv\n",
    "\n",
    "    temp_conv = batch_norm('bn_1', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    #conv2\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 128], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 128], stride=2)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = batch_norm('bn_2', temp_conv,train=is_training)\n",
    "    \n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    #conv3\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 256], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 256], stride=1)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 256], stride=2)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = batch_norm('bn_3', temp_conv, train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    \n",
    "    #conv4\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 512], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    \n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = batch_norm('bn_4', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "\n",
    "    #conv5\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = batch_norm('bn_5', temp_conv,train=is_training)\n",
    "    \n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    #conv6\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = batch_norm('bn_6', temp_conv,train=is_training) \n",
    "    \n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    #conv7\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = batch_norm('bn_7', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    \n",
    "    #Dropout layer for OF\n",
    "    temp_conv = tf.nn.dropout(temp_conv, keep_prob)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    \n",
    "    #conv8\n",
    "    temp_conv = conv2d_transpose_strided('conv' + str(conv_num), temp_conv, [4, 4, 256, 512], stride=2)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 256], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 256], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = batch_norm('bn_8', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "\n",
    "    \n",
    "    #conv9\n",
    "    temp_conv = conv2d_transpose_strided('conv' + str(conv_num), temp_conv, [3, 3, 128, 256], stride=2)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 128], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 128], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = batch_norm('bn_9', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    #conv10\n",
    "    temp_conv = conv2d_transpose_strided('conv' + str(conv_num), temp_conv, [3, 3, 64, 128], stride=2)\n",
    "    conv_num += 1    \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 64], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 64], stride=1)\n",
    "    conv_num += 1\n",
    "    \n",
    "    temp_conv = batch_norm('bn_10', temp_conv,train=is_training)\n",
    "    \n",
    "    print(temp_conv.get_shape())\n",
    "    \n",
    "    W_last = weight_variable([3, 3, 64, 3], 'conv_W_last')\n",
    "    b_last = bias_variable([3], 'conv_b_last')\n",
    "    last_cnn = tf.nn.sigmoid(tf.nn.conv2d(temp_conv, W_last, strides=[1, 1, 1, 1], padding='SAME') )\n",
    "    print(last_cnn.get_shape())\n",
    "    \n",
    "    \n",
    "    _lambda = 0.05;\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(scale=_lambda, scope=None)\n",
    "    regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "    \n",
    "    \n",
    "    #Loss computation\n",
    "    gen_loss_mse = tf.reduce_mean(2 * tf.nn.l2_loss(last_cnn - y)) + regularization_penalty\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(gen_loss_mse)\n",
    "    \n",
    "    #pred image\n",
    "    tf.add_to_collection('optimizer', optimizer)  \n",
    "    tf.add_to_collection('loss', gen_loss_mse) \n",
    "    tf.add_to_collection('color_image', last_cnn)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #Training\n",
    "    for itr in range(hm_epochs):\n",
    "        #image, color_images = get_next_batch(train_features, train_labels)\n",
    "        image, color_images = get_next_batch_from_disk_RGB_Gaussian(images_list=images_list, batch_size=batch_size, crop_size=224)\n",
    "        #print(image.shape)\n",
    "        feed_dict = {x: image, y: color_images, keep_prob: dropout_keep_prob, is_training : True}\n",
    "        \n",
    "\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if itr % 20 == 0:\n",
    "            \n",
    "            image_test, color_images_test = get_next_batch_from_disk_RGB_Gaussian(images_list=images_list_test, batch_size=batch_size, crop_size=224)\n",
    "            feed_dict_test = {x: image_test, y: color_images_test, keep_prob: 1.0, is_training : False}\n",
    "            \n",
    "            mse = sess.run(gen_loss_mse, feed_dict=feed_dict_test)\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            print(\"Step: %d, MSE: %g\" % (itr, mse))\n",
    "     \n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 112, 112, 64)\n",
      "(15, 112, 112, 64)\n",
      "(15, 56, 56, 128)\n",
      "(15, 28, 28, 256)\n",
      "(15, 28, 28, 512)\n",
      "(15, 28, 28, 512)\n",
      "(15, 28, 28, 512)\n",
      "(15, 28, 28, 512)\n",
      "(15, 28, 28, 512)\n",
      "(15, 56, 56, 256)\n",
      "(15, 112, 112, 128)\n",
      "(15, 224, 224, 64)\n",
      "(15, 224, 224, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-05f28dd36b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_neural_network_colorization_nilboyarch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-95277edff5e7>\u001b[0m in \u001b[0;36mtrain_neural_network_colorization_nilboyarch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[1;31m#Loss computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mgen_loss_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_cnn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mregularization_penalty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[1;31m#pred image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[1;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 \u001b[0min_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    612\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   return (array_ops.reshape(math_ops.reduce_sum(grad * y, rx), sx),\n\u001b[0;32m--> 614\u001b[0;31m           array_ops.reshape(math_ops.reduce_sum(x * grad, ry), sy))\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   2628\u001b[0m   \"\"\"\n\u001b[1;32m   2629\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[0;32m-> 2630\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2631\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1755\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    668\u001b[0m       output = pywrap_tensorflow.RunCppShapeInference(\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No shape inference function exists for op\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Global constants\n",
    "seed = 56\n",
    "\n",
    "#cycles of feed forward + backprop on all K-folded samples\n",
    "hm_epochs = 40\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "model_path = \"./recent2/model.ckpt\"\n",
    "save_dir = './recent2/'\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 224, 224, 1], name = 'x')\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, 224, 224, 3], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "is_training = tf.placeholder(tf.bool, name = 'is_training');\n",
    "#dropout_prob = tf.placeholder('float', (), name = 'dropout_prob')\n",
    "\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "train_neural_network_colorization_nilboyarch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape, name, dropout=1):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=seed, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides=[1,1,1,1]):\n",
    "    return tf.nn.conv2d(x, W, strides=strides, padding='SAME')\n",
    "\n",
    "#From https://github.com/shekkizh/Colorization.tensorflow/blob/master/TensorflowUtils.py\n",
    "def conv2d_transpose_strided(x, W, output_shape=None, stride=2):\n",
    "    #print (x.get_shape())\n",
    "    #print (W.get_shape())\n",
    "    if output_shape is None:\n",
    "        output_shape = x.get_shape().as_list()\n",
    "        output_shape[1] *= 2\n",
    "        output_shape[2] *= 2\n",
    "        output_shape[3] = W.get_shape().as_list()[2]\n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "def batch_norm(scope, x, train=True, reuse=False):\n",
    "    return tf.contrib.layers.batch_norm(x, center=True, scale=True, updates_collections=None, is_training=train, trainable=True, scope=scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network_colorization_sig(x):\n",
    "    \n",
    "    strides1 = [1, 1, 1, 1]\n",
    "    strides2 = [1, 2, 2, 1]\n",
    "    \n",
    "    # first convolutional layer\n",
    "    W_conv1 = weight_variable([3, 3, 1, 64], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([64], \"b_conv1\")\n",
    "\n",
    "    h_conv1 = tf.nn.relu6(conv2d(x, W_conv1, strides2) + b_conv1)\n",
    "    \n",
    "    print(h_conv1.get_shape())\n",
    "    \n",
    "    #Second CNN layer\n",
    "    W_conv2 = weight_variable([3, 3, 64, 128], \"W_conv2\")\n",
    "    b_conv2 = bias_variable([128], \"b_conv2\")\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "    \n",
    "    print(h_conv2.get_shape())\n",
    "    \n",
    "    #Third CNN Layer\n",
    "    W_conv3 = weight_variable([3, 3, 128, 128], \"W_conv3\")\n",
    "    b_conv3 = bias_variable([128], \"b_conv3\")\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, strides2) + b_conv3)\n",
    "    \n",
    "    print(h_conv3.get_shape())\n",
    "    \n",
    "    #Fourth CNN Layer\n",
    "    W_conv4 = weight_variable([3, 3, 128, 256], \"W_conv4\")\n",
    "    b_conv4 = bias_variable([256], \"b_conv4\")\n",
    "\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4 ) + b_conv4)\n",
    "    \n",
    "    print(h_conv4.get_shape())\n",
    "    \n",
    "    #Fifth CNN Layer\n",
    "    W_conv5 = weight_variable([3, 3, 256, 256], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([256], \"b_conv5\")\n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, strides2) + b_conv5)\n",
    "    \n",
    "    print(h_conv5.get_shape())\n",
    "    \n",
    "    #Sixth CNN Layer\n",
    "    W_conv6 = weight_variable([3, 3, 256, 512], \"W_conv6\")\n",
    "    b_conv6 = bias_variable([512], \"b_conv6\")\n",
    "\n",
    "    h_conv6 = tf.nn.relu(conv2d(h_conv5, W_conv6) + b_conv6)\n",
    "    \n",
    "    print(h_conv6.get_shape())\n",
    "    \n",
    "    #Mid Level features\n",
    "    W_midlvl1 = weight_variable([3, 3, 512, 512], \"W_midlvl1\")\n",
    "    b_midlvl1 = bias_variable([512], \"b_midlvl1\")\n",
    "\n",
    "    h_midlvl1 = tf.nn.relu(conv2d(h_conv6, W_midlvl1) + b_midlvl1)\n",
    "    #h_pool_midlvl1 = max_pool_2x2(h_midlvl1)\n",
    "\n",
    "    print(h_midlvl1.get_shape())\n",
    "    \n",
    "    W_midlvl2 = weight_variable([3, 3, 512, 256], \"W_midlvl2\")\n",
    "    b_midlvl2 = bias_variable([256], \"b_midlvl2\")\n",
    "\n",
    "    h_midlvl2 = tf.nn.relu(conv2d(h_midlvl1, W_midlvl2) + b_midlvl2)\n",
    "\n",
    "    print(h_midlvl2.get_shape())\n",
    "    \n",
    "    #Dropout layer\n",
    "    h_drop = tf.nn.dropout(h_midlvl2, keep_prob)\n",
    "\n",
    "    #Colorization Network\n",
    "    \n",
    "    #Transposed CNN Layer 1\n",
    "    W_transpose_conv1 = weight_variable([3, 3, 128, 256], \"W_transpose_conv1\")\n",
    "    b_transpose_conv1 = bias_variable([128], \"b_transpose_conv1\")\n",
    "\n",
    "    h_conv_t_4 = conv2d_transpose_strided(h_drop, W_transpose_conv1) + b_transpose_conv1\n",
    "    \n",
    "    print(h_conv_t_4.get_shape())\n",
    "    \n",
    "    # CNN conv\n",
    "    \n",
    "    W_colorizer_CNN1 = weight_variable([3, 3, 128 , 128 ], \"W_colorizer_CNN1\")\n",
    "    b_colorizer_CNN1 = bias_variable([128 ], \"b_colorizer_CNN1\")\n",
    "\n",
    "    h_colorizer_conv1 = tf.nn.relu6(conv2d(h_conv_t_4, W_colorizer_CNN1) + b_colorizer_CNN1)\n",
    "    \n",
    "    print(h_colorizer_conv1.get_shape())\n",
    "\n",
    "    #Transposed CNN Layer 2\n",
    "    W_transpose_conv2 = weight_variable([3, 3, 64, 128], \"W_transpose_conv2\")\n",
    "    b_transpose_conv2 = bias_variable([64], \"b_transpose_conv2\")\n",
    "\n",
    "    h_conv_t_5 = conv2d_transpose_strided(h_colorizer_conv1, W_transpose_conv2) + b_transpose_conv2\n",
    "    \n",
    "    print(h_conv_t_5.get_shape())\n",
    "      \n",
    "    #Transposed CNN Layer 3\n",
    "    W_transpose_conv3 = weight_variable([3, 3, 3, 64], \"W_transpose_conv3\")\n",
    "    b_transpose_conv3 = bias_variable([3], \"b_transpose_conv3\")\n",
    "\n",
    "    h_conv_t_6 = tf.add(conv2d_transpose_strided(h_conv_t_5, W_transpose_conv3), b_transpose_conv3)\n",
    "    \n",
    "    print(h_conv_t_6.get_shape())\n",
    "    \n",
    "    W_last = weight_variable([3, 3, 3, 3], \"W_last\")\n",
    "    b_last = bias_variable([3], \"b_last\")\n",
    "    last_cnn = tf.nn.sigmoid(conv2d(h_conv_t_6, W_last) + b_last, name=\"color_image\")\n",
    "    \n",
    "    print(last_cnn.get_shape())\n",
    "    \n",
    "    \n",
    "    #to_save = [W_conv1, b_conv1, W_conv2, b_conv2, W_conv3, b_conv3, W_transpose_conv1, b_transpose_conv1, W_transpose_conv2, b_transpose_conv2, W_transpose_conv3, b_transpose_conv3]    \n",
    "    \n",
    "    '''_lambda = 0;\n",
    "  \n",
    "    \n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(scale=_lambda, scope=None)\n",
    "   \n",
    "    weights = [W_conv1, W_conv2, W_conv3, W_conv4, W_conv5, W_conv6, W_midlvl1, W_midlvl2,\n",
    "                    W_transpose_conv1, W_colorizer_CNN1, W_transpose_conv2, W_transpose_conv3, W_last]\n",
    "    \n",
    "    regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "    '''\n",
    "    \n",
    "    #Loss computation\n",
    "    gen_loss_mse = tf.reduce_mean(2 * tf.nn.l2_loss(last_cnn - y)) #+ regularization_penalty\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(gen_loss_mse)\n",
    "    \n",
    "    #pred image\n",
    "    tf.add_to_collection('optimizer', optimizer)  \n",
    "    tf.add_to_collection('loss', gen_loss_mse) \n",
    "    tf.add_to_collection('color_image', last_cnn)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #Training\n",
    "    for itr in range(hm_epochs):\n",
    "        #image, color_images = get_next_batch(train_features, train_labels)\n",
    "        image, color_images = get_next_batch_from_disk_RGB(images_list=images_list, batch_size=batch_size)\n",
    "        #print(image.shape)\n",
    "        feed_dict = {x: image, y: color_images, keep_prob: dropout_keep_prob}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if itr % 100 == 0:\n",
    "            mse = sess.run(gen_loss_mse, feed_dict=feed_dict)\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            print(\"Step: %d, MSE: %g\" % (itr, mse))\n",
    "     \n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 32, 32, 64)\n",
      "(80, 32, 32, 128)\n",
      "(80, 16, 16, 128)\n",
      "(80, 16, 16, 256)\n",
      "(80, 8, 8, 256)\n",
      "(80, 8, 8, 512)\n",
      "(80, 8, 8, 512)\n",
      "(80, 8, 8, 256)\n",
      "(80, 16, 16, 128)\n",
      "(80, 16, 16, 128)\n",
      "(80, 32, 32, 64)\n",
      "(80, 64, 64, 3)\n",
      "(80, 64, 64, 3)\n",
      "Step: 0, MSE: 163609\n",
      "Step: 100, MSE: 58441.2\n",
      "Step: 200, MSE: 54213.9\n",
      "Step: 300, MSE: 53787.1\n",
      "Step: 400, MSE: 56334.3\n",
      "Step: 500, MSE: 51149.2\n",
      "Step: 600, MSE: 56110.1\n",
      "Step: 700, MSE: 49822.7\n",
      "Step: 800, MSE: 52687.2\n",
      "Step: 900, MSE: 51984.4\n"
     ]
    }
   ],
   "source": [
    "#Global constants\n",
    "seed = 56\n",
    "\n",
    "#cycles of feed forward + backprop on all K-folded samples\n",
    "hm_epochs = 1000\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "model_path = \"./test3/model.ckpt\"\n",
    "save_dir = './test3/'\n",
    "\n",
    "batch_size = 80\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 64, 64, 1], name = 'x')\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, 64, 64, 3], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "#dropout_prob = tf.placeholder('float', (), name = 'dropout_prob')\n",
    "\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "train_neural_network_colorization_sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 182, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, color_images = get_next_batch_from_disk_RGB(images_list=images_list, batch_size=batch_size)\n",
    "color_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#tf.contrib.layers.apply_regularization(tf.contrib.layers.l1_regularizer(_lambda), weights_list=\n",
    "#                                     [W_conv1, W_conv2, ])\n",
    "#vars1  = tf.trainable_variables() \n",
    "#lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars1\n",
    "            #if 'bias' not in v.name ]) * _lambda\n",
    "\n",
    "\n",
    "#weights = [v for v in tf.trainable_variables() if 'bias' not in v.name]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network_colorization_sig2(x):\n",
    "    \n",
    "    strides1 = [1, 1, 1, 1]\n",
    "    strides2 = [1, 2, 2, 1]\n",
    "    \n",
    "    # first convolutional layer\n",
    "    W_conv1 = weight_variable([3, 3, 1, 64], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([64], \"b_conv1\")\n",
    "\n",
    "    h_conv1 = tf.nn.relu6(conv2d(x, W_conv1, strides2) + b_conv1)\n",
    "    \n",
    "    print(h_conv1.get_shape())\n",
    "    \n",
    "    #Second CNN layer\n",
    "    W_conv2 = weight_variable([3, 3, 64, 128], \"W_conv2\")\n",
    "    b_conv2 = bias_variable([128], \"b_conv2\")\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "    \n",
    "    print(h_conv2.get_shape())\n",
    "    \n",
    "    #Third CNN Layer\n",
    "    W_conv3 = weight_variable([3, 3, 128, 128], \"W_conv3\")\n",
    "    b_conv3 = bias_variable([128], \"b_conv3\")\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, strides2) + b_conv3)\n",
    "    \n",
    "    print(h_conv3.get_shape())\n",
    "    \n",
    "    #Fourth CNN Layer\n",
    "    W_conv4 = weight_variable([3, 3, 128, 256], \"W_conv4\")\n",
    "    b_conv4 = bias_variable([256], \"b_conv4\")\n",
    "\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4 ) + b_conv4)\n",
    "    \n",
    "    print(h_conv4.get_shape())\n",
    "    \n",
    "    #Fifth CNN Layer\n",
    "    W_conv5 = weight_variable([3, 3, 256, 256], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([256], \"b_conv5\")\n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, strides2) + b_conv5)\n",
    "    \n",
    "    print(h_conv5.get_shape())\n",
    "    \n",
    "    #Sixth CNN Layer\n",
    "    W_conv6 = weight_variable([3, 3, 256, 512], \"W_conv6\")\n",
    "    b_conv6 = bias_variable([512], \"b_conv6\")\n",
    "\n",
    "    h_conv6 = tf.nn.relu(conv2d(h_conv5, W_conv6) + b_conv6)\n",
    "    \n",
    "    print(h_conv6.get_shape())\n",
    "    \n",
    "    #Mid Level features\n",
    "    W_midlvl1 = weight_variable([3, 3, 512, 512], \"W_midlvl1\")\n",
    "    b_midlvl1 = bias_variable([512], \"b_midlvl1\")\n",
    "\n",
    "    h_midlvl1 = tf.nn.relu(conv2d(h_conv6, W_midlvl1) + b_midlvl1)\n",
    "    #h_pool_midlvl1 = max_pool_2x2(h_midlvl1)\n",
    "\n",
    "    print(h_midlvl1.get_shape())\n",
    "    \n",
    "    W_midlvl2 = weight_variable([3, 3, 512, 256], \"W_midlvl2\")\n",
    "    b_midlvl2 = bias_variable([256], \"b_midlvl2\")\n",
    "\n",
    "    h_midlvl2 = tf.nn.relu(conv2d(h_midlvl1, W_midlvl2) + b_midlvl2)\n",
    "\n",
    "    print(h_midlvl2.get_shape())\n",
    "\n",
    "    #Colorization Network\n",
    "    \n",
    "    #Upsample\n",
    "    h_conv_t_4 = tf.image.resize_images(h_midlvl2, [64, 64], method=1) #1=nearest neighbour\n",
    "    print(\"rs 1\")\n",
    "    print(h_conv_t_4.get_shape())\n",
    "    \n",
    "    # CNN conv\n",
    "    \n",
    "    W_colorizer_CNN1 = weight_variable([3, 3, 256 , 128], \"W_colorizer_CNN1\")\n",
    "    b_colorizer_CNN1 = bias_variable([128], \"b_colorizer_CNN1\")\n",
    "\n",
    "    h_colorizer_conv1 = tf.nn.relu6(conv2d(h_conv_t_4, W_colorizer_CNN1) + b_colorizer_CNN1)\n",
    "    \n",
    "    print(h_colorizer_conv1.get_shape())\n",
    "    \n",
    "    \n",
    "    #Upsample\n",
    "    h_conv_t_5 = tf.image.resize_images(h_colorizer_conv1, [128, 128], method=1) #1=nearest neighbour\n",
    "    print(\"rs 2\")\n",
    "    print(h_conv_t_5.get_shape())\n",
    "    \n",
    "    \n",
    "    # CNN conv\n",
    "    W_colorizer_CNN2 = weight_variable([3, 3, 128 , 64], \"W_colorizer_CNN2\")\n",
    "    b_colorizer_CNN2 = bias_variable([64], \"b_colorizer_CNN2\")\n",
    "\n",
    "    h_colorizer_conv2 = tf.nn.relu6(conv2d(h_conv_t_5, W_colorizer_CNN2) + b_colorizer_CNN2)\n",
    "    \n",
    "    print(h_colorizer_conv2.get_shape())\n",
    "    \n",
    "    \n",
    "    #Upsample\n",
    "    h_conv_t_6 = tf.image.resize_images(h_colorizer_conv2, [256, 256], method=1) #1=nearest neighbour\n",
    "    print(\"rs 3\")\n",
    "    print(h_conv_t_6.get_shape())\n",
    "    \n",
    "    # CNN conv\n",
    "    W_colorizer_CNN3 = weight_variable([3, 3, 64 , 3], \"W_colorizer_CNN3\")\n",
    "    b_colorizer_CNN3 = bias_variable([3], \"b_colorizer_CNN3\")\n",
    "\n",
    "    h_colorizer_conv3 = tf.nn.relu6(conv2d(h_conv_t_6, W_colorizer_CNN3) + b_colorizer_CNN3)\n",
    "    print(h_colorizer_conv3.get_shape())\n",
    "    \n",
    "    \n",
    "    W_last = weight_variable([3, 3, 3, 3], \"W_last\")\n",
    "    b_last = bias_variable([3], \"b_last\")\n",
    "    last_cnn = tf.nn.sigmoid(conv2d(h_colorizer_conv3, W_last) + b_last, name=\"color_image\")\n",
    "    \n",
    "    \n",
    "    print(last_cnn.get_shape())\n",
    "\n",
    "    #to_save = [W_conv1, b_conv1, W_conv2, b_conv2, W_conv3, b_conv3, W_transpose_conv1, b_transpose_conv1, W_transpose_conv2, b_transpose_conv2, W_transpose_conv3, b_transpose_conv3]    \n",
    "    \n",
    "    _lambda = 0.005;\n",
    "  \n",
    "    \n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(scale=_lambda, scope=None)\n",
    "   \n",
    "    weights = [W_conv1, W_conv2, W_conv3, W_conv4, W_conv5, W_conv6, W_midlvl1, W_midlvl2,\n",
    "                    W_colorizer_CNN1, W_colorizer_CNN2,\n",
    "                        W_colorizer_CNN3, W_last]\n",
    "    \n",
    "    regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "    \n",
    "    #Loss computation\n",
    "    gen_loss_mse = tf.reduce_mean(2 * tf.nn.l2_loss(last_cnn - y)) + regularization_penalty\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(gen_loss_mse)\n",
    "    \n",
    "    #pred image\n",
    "    tf.add_to_collection('optimizer', optimizer)  \n",
    "    tf.add_to_collection('loss', gen_loss_mse) \n",
    "    tf.add_to_collection('color_image', last_cnn)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #Training\n",
    "    for itr in range(hm_epochs):\n",
    "        #image, color_images = get_next_batch(train_features, train_labels)\n",
    "        image, color_images = get_next_batch_from_disk_RGB_Nocrop(images_list=images_list, batch_size=batch_size)\n",
    "        #print(image.shape)\n",
    "        feed_dict = {x: image, y: color_images}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if itr % 100 == 0:\n",
    "            mse = sess.run(gen_loss_mse, feed_dict=feed_dict)\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            print(\"Step: %d, MSE: %g\" % (itr, mse))\n",
    "     \n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 64)\n",
      "(1, 128, 128, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 256)\n",
      "(1, 32, 32, 256)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 256)\n",
      "rs 1\n",
      "(1, 64, 64, 256)\n",
      "(1, 64, 64, 128)\n",
      "rs 2\n",
      "(1, 128, 128, 128)\n",
      "(1, 128, 128, 64)\n",
      "rs 3\n",
      "(1, 256, 256, 64)\n",
      "(1, 256, 256, 3)\n",
      "(1, 256, 256, 3)\n",
      "Step: 0, MSE: 8138.69\n",
      "Step: 100, MSE: 6891.7\n",
      "Step: 200, MSE: 6383.14\n",
      "Step: 300, MSE: 6104.76\n",
      "Step: 400, MSE: 5930.78\n",
      "Step: 500, MSE: 5818.62\n",
      "Step: 600, MSE: 5740.91\n",
      "Step: 700, MSE: 5680.94\n",
      "Step: 800, MSE: 5633.58\n",
      "Step: 900, MSE: 5586.99\n"
     ]
    }
   ],
   "source": [
    "#Global constants\n",
    "seed = 56\n",
    "\n",
    "#cycles of feed forward + backprop on all K-folded samples\n",
    "hm_epochs = 1000\n",
    "\n",
    "model_path = \"./test5/model.ckpt\"\n",
    "save_dir = './test5/'\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 256, 256, 1], name = 'x')\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, 256, 256, 3], name='y')\n",
    "#dropout_prob = tf.placeholder('float', (), name = 'dropout_prob')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "train_neural_network_colorization_sig2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def train_neural_network_colorization_sig(x):\n",
    "    \n",
    "    strides1 = [1, 1, 1, 1]\n",
    "    strides2 = [1, 2, 2, 1]\n",
    "    \n",
    "    # first convolutional layer\n",
    "    W_conv1 = weight_variable([3, 3, 1, 64], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([64], \"b_conv1\")\n",
    "\n",
    "    h_conv1 = tf.nn.relu6(conv2d(x, W_conv1, strides2) + b_conv1)\n",
    "    \n",
    "    print(h_conv1.get_shape())\n",
    "    \n",
    "    #Second CNN layer\n",
    "    W_conv2 = weight_variable([3, 3, 64, 128], \"W_conv2\")\n",
    "    b_conv2 = bias_variable([128], \"b_conv2\")\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "    \n",
    "    print(h_conv2.get_shape())\n",
    "    \n",
    "    #Third CNN Layer\n",
    "    W_conv3 = weight_variable([3, 3, 128, 128], \"W_conv3\")\n",
    "    b_conv3 = bias_variable([128], \"b_conv3\")\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, strides2) + b_conv3)\n",
    "    \n",
    "    print(h_conv3.get_shape())\n",
    "    \n",
    "    #Fourth CNN Layer\n",
    "    W_conv4 = weight_variable([3, 3, 128, 256], \"W_conv4\")\n",
    "    b_conv4 = bias_variable([256], \"b_conv4\")\n",
    "\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4 ) + b_conv4)\n",
    "    \n",
    "    print(h_conv4.get_shape())\n",
    "    \n",
    "    #Fifth CNN Layer\n",
    "    W_conv5 = weight_variable([3, 3, 256, 256], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([256], \"b_conv5\")\n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, strides2) + b_conv5)\n",
    "    \n",
    "    print(h_conv5.get_shape())\n",
    "    \n",
    "    #Sixth CNN Layer\n",
    "    W_conv6 = weight_variable([3, 3, 256, 512], \"W_conv6\")\n",
    "    b_conv6 = bias_variable([512], \"b_conv6\")\n",
    "\n",
    "    h_conv6 = tf.nn.relu(conv2d(h_conv5, W_conv6) + b_conv6)\n",
    "    \n",
    "    print(h_conv6.get_shape())\n",
    "    \n",
    "    #Mid Level features\n",
    "    W_midlvl1 = weight_variable([3, 3, 512, 512], \"W_midlvl1\")\n",
    "    b_midlvl1 = bias_variable([512], \"b_midlvl1\")\n",
    "\n",
    "    h_midlvl1 = tf.nn.relu(conv2d(h_conv6, W_midlvl1) + b_midlvl1)\n",
    "    #h_pool_midlvl1 = max_pool_2x2(h_midlvl1)\n",
    "\n",
    "    print(h_midlvl1.get_shape())\n",
    "    \n",
    "    W_midlvl2 = weight_variable([3, 3, 512, 256], \"W_midlvl2\")\n",
    "    b_midlvl2 = bias_variable([256], \"b_midlvl2\")\n",
    "\n",
    "    h_midlvl2 = tf.nn.relu(conv2d(h_midlvl1, W_midlvl2) + b_midlvl2)\n",
    "\n",
    "    print(h_midlvl2.get_shape())\n",
    "\n",
    "    #Colorization Network\n",
    "    \n",
    "    #Transposed CNN Layer 1\n",
    "    W_transpose_conv1 = weight_variable([3, 3, 128, 256], \"W_transpose_conv1\")\n",
    "    b_transpose_conv1 = bias_variable([128], \"b_transpose_conv1\")\n",
    "\n",
    "    h_conv_t_4 = conv2d_transpose_strided(h_midlvl2, W_transpose_conv1) + b_transpose_conv1\n",
    "    \n",
    "    print(h_conv_t_4.get_shape())\n",
    "    \n",
    "    # CNN conv\n",
    "    \n",
    "    W_colorizer_CNN1 = weight_variable([1, 1, 128 , 128], \"W_colorizer_CNN1\")\n",
    "    b_colorizer_CNN1 = bias_variable([128], \"b_colorizer_CNN1\")\n",
    "\n",
    "    h_colorizer_conv1 = tf.nn.relu6(conv2d(h_conv_t_4, W_colorizer_CNN1) + b_colorizer_CNN1)\n",
    "    \n",
    "    print(h_colorizer_conv1.get_shape())\n",
    "\n",
    "    #Transposed CNN Layer 2\n",
    "    W_transpose_conv2 = weight_variable([3, 3, 64, 128], \"W_transpose_conv2\")\n",
    "    b_transpose_conv2 = bias_variable([64], \"b_transpose_conv2\")\n",
    "\n",
    "    h_conv_t_5 = conv2d_transpose_strided(h_colorizer_conv1, W_transpose_conv2) + b_transpose_conv2\n",
    "    \n",
    "    print(h_conv_t_5.get_shape())\n",
    "    \n",
    "    \n",
    "    # CNN conv\n",
    "    \n",
    "    W_colorizer_CNN2 = weight_variable([1, 1, 64 , 64], \"W_colorizer_CNN2\")\n",
    "    b_colorizer_CNN2 = bias_variable([64], \"b_colorizer_CNN2\")\n",
    "\n",
    "    h_colorizer_conv2 = tf.nn.relu6(conv2d(h_conv_t_5, W_colorizer_CNN2) + b_colorizer_CNN2)\n",
    "    \n",
    "    print(h_colorizer_conv2.get_shape())\n",
    "    \n",
    "      \n",
    "    #Transposed CNN Layer 3\n",
    "    W_transpose_conv3 = weight_variable([3, 3, 32, 64], \"W_transpose_conv3\")\n",
    "    b_transpose_conv3 = bias_variable([32], \"b_transpose_conv3\")\n",
    "\n",
    "    h_conv_t_6 = tf.add(conv2d_transpose_strided(h_colorizer_conv2, W_transpose_conv3), b_transpose_conv3)\n",
    "    \n",
    "    \n",
    "    # CNN conv\n",
    "    W_colorizer_CNN3 = weight_variable([1, 1, 32, 3], \"W_colorizer_CNN3\")\n",
    "    b_colorizer_CNN3 = bias_variable([3], \"b_colorizer_CNN3\")\n",
    "\n",
    "    h_colorizer_conv3 = tf.nn.relu6(conv2d(h_conv_t_6, W_colorizer_CNN3) + b_colorizer_CNN3)\n",
    "    \n",
    "    print(h_colorizer_conv3.get_shape())\n",
    "    \n",
    "    \n",
    "    W_last = weight_variable([1, 1, 3, 3], \"W_last\")\n",
    "    b_last = bias_variable([3], \"b_last\")\n",
    "    last_cnn = tf.nn.sigmoid(conv2d(h_colorizer_conv3, W_last) + b_last, name=\"color_image\")\n",
    "    \n",
    "    \n",
    "    print(last_cnn.get_shape())\n",
    "\n",
    "    #to_save = [W_conv1, b_conv1, W_conv2, b_conv2, W_conv3, b_conv3, W_transpose_conv1, b_transpose_conv1, W_transpose_conv2, b_transpose_conv2, W_transpose_conv3, b_transpose_conv3]    \n",
    "    \n",
    "    _lambda = 0.005;\n",
    "  \n",
    "    \n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(scale=_lambda, scope=None)\n",
    "   \n",
    "    weights = [W_conv1, W_conv2, W_conv3, W_conv4, W_conv5, W_conv6, W_midlvl1, W_midlvl2,\n",
    "                    W_transpose_conv1, W_colorizer_CNN1, W_transpose_conv2, W_colorizer_CNN2, W_transpose_conv3,\n",
    "                        W_colorizer_CNN3, W_last]\n",
    "    \n",
    "    regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, weights)\n",
    "    \n",
    "    #Loss computation\n",
    "    gen_loss_mse = tf.reduce_mean(2 * tf.nn.l2_loss(last_cnn - y)) + regularization_penalty\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(gen_loss_mse)\n",
    "    \n",
    "    #pred image\n",
    "    tf.add_to_collection('optimizer', optimizer)  \n",
    "    tf.add_to_collection('loss', gen_loss_mse) \n",
    "    tf.add_to_collection('color_image', last_cnn)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #Training\n",
    "    for itr in range(hm_epochs):\n",
    "        #image, color_images = get_next_batch(train_features, train_labels)\n",
    "        image, color_images = get_next_batch_from_disk_RGB_Nocrop(images_list=images_list, batch_size=batch_size)\n",
    "        #print(image.shape)\n",
    "        feed_dict = {x: image, y: color_images}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if itr % 100 == 0:\n",
    "            mse = sess.run(gen_loss_mse, feed_dict=feed_dict)\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            print(\"Step: %d, MSE: %g\" % (itr, mse))\n",
    "     \n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    sess.close()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
